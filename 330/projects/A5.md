# C330 A5 - Logistic Regression

### Peter Dulworth

##### I trained on the largest (1.9GB) data set and tested on the medium (200 MB) data set

### Task 1

Task 1 was trained on the largest data set.

| word      | rank |
| --------- | ---- |
| applicant | 448  |
| and       | 2    |
| attack    | 514  |
| protein   | 3167 |
| car       | 652  |

Raw Output:

```bash
>>> wordsToRanks.lookup("applicant")
448
>>> wordsToRanks.lookup("and")
2
>>> wordsToRanks.lookup("attack")
514
>>> wordsToRanks.lookup("protein")
3167
>>> wordsToRanks.lookup("car")
652
```

### Task 2

**Gradient Formula**

Loss function with regularization (negative of LLH):
$$
L = -\sum_i(y_i(x_i\cdot r)-log(1 + e^{x_i\cdot r})-\lambda(||r||_2)^2)
$$
The gradient is the vector 
$$
\nabla L = \begin{bmatrix}\frac{\partial L}{\partial r_1} \cdots \frac{\partial L}{\partial r_k} \cdots \frac{\partial L}{\partial r_n}\end{bmatrix}
$$


Where $n$ is the length of $x_i$ (or the number of features) and $\frac{\partial L}{\partial r_k}$ is
$$
\frac{\partial L}{\partial r_k} = -\sum_i(y_ix_{ik}-\frac{x_{ik}e^{x_i\cdot r}}{1 + e^{x_i\cdot r}} - 2 \lambda r_k)
$$
**Fifty Words with Largest Regression Coefficients**

*These are the fifty words that are most strongly related with an Australian court case.*

'pty', 'hca', 'clr', 'mr', 'affidavit', 'said', 'relation', 'whether', 'pursuant', 'submissions', 'purposes', 'proceeding', 'reasons', 'fcr', 'claim', 'satisfied', 'particular', 'respect', 'accordance', 'ms', 'circumstances', 'am', 'amp', 'dr', 'fca', 'honour', 'evidence', 'i', 'relevantly', 'applicant', 'regard', 'my', 'consideration', 'orders', 'view', 'relevant', 'submitted', 'ltd', 'accept', 'determination', 'notice', 'reference', 'scheme', 'referred', 'matters', 'native', 'applicants', 'memorandum', 'j', 'conduct'

### Task 3

All the testing was done on the 200MB data set. I got the best $F_1$ score with $\lambda=0.0$. The code for the regularization and normalization is submitted as part of my python file however $\lambda$ is set to $0.0$ and the normalization is commented out. Normalization code is from line 178-202 for training data and from line 325-334 for testing data.

**F1 Score**

$F_1 = 0.9946949602122016$

**False Positives**

When classifying the testing data, I only had two false positives so I will report only those. The false positives where for documents $14683768$ and $13442178$.

Document $14683768$ is a wikipedia article about the "Hog's breath cafe" an "Australian chain of steak house restaurants." One factor that I believe contributed to this document fooling my model was definitely that the article was about an Australian resaurant. The article also mentions a legal dispute which could be another factor. Additionally the words: 'particular', 'referred', 'mr', 'relation',  'reasons', and 'determination' all appear in this article and they are also all in the top 50 words from task 2. Document $13442178$ is a wikipedia article that contains a list of characters from a book. Although the article isn't about Australia or the legal system, because it is a list of words a high percentage of the words are "dr" and "mr" which are both words in the top fifty from task 2. Overall there are 43 occurences of "mr" and "dr".